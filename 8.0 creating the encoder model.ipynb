{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4909cd4d-d4fd-4b32-b166-83e3d58ad992",
   "metadata": {},
   "source": [
    "Step 1: Install/Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "id": "77486608-7220-406c-bd5a-0690497eef90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T07:42:14.585094Z",
     "start_time": "2025-02-03T07:42:07.344648Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "00597588-7ac5-4cdc-bc9f-3f337ee866dc",
   "metadata": {},
   "source": [
    "2. Load & Preprocess the CSV"
   ]
  },
  {
   "cell_type": "code",
   "id": "5a7004b0-5799-4881-b73f-7d8bf313a264",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T07:42:47.531790Z",
     "start_time": "2025-02-03T07:42:18.786073Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# STEP 2.1: Load the CSV\n",
    "df = pd.read_csv(r\"C:\\Users\\SADAT\\Desktop\\university\\CoE_4\\Final Project\\our coding files\\PRACTICE DATA\\cleaned_vedio_dataset_version2.csv\")\n",
    "\n",
    "# Inspect columns\n",
    "print(df.columns)\n",
    "\n",
    "# Example columns: \n",
    "# ['label', 'hand_1_0_x', 'hand_1_0_y', ..., 'hand_2_20_z']"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "63698c1f-e239-46c1-aa64-edcc3ae20ac4",
   "metadata": {},
   "source": [
    "2.2: Separate Labels & Features"
   ]
  },
  {
   "cell_type": "code",
   "id": "a87f9523-4410-4e62-be6b-3012623a7b34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T07:42:56.482295Z",
     "start_time": "2025-02-03T07:42:54.384830Z"
    }
   },
   "source": [
    "# Drop the 'label' to get numeric columns only\n",
    "feature_cols = [c for c in df.columns if c != 'label']\n",
    "X = df[feature_cols].values  # shape: [num_samples, 126] if 2 hands\n",
    "\n",
    "# Get the labels\n",
    "y = df['label'].values  # shape: [num_samples]\n"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d75e1039-7062-4f9e-a88d-f6ce92ba9e61",
   "metadata": {},
   "source": [
    "2.3: Handle Missing or Zero-Filled Hands:\n",
    "    Some rows have all zeros for the second hand. You can either:\n",
    "    Keep them as is (the model learns “one-hand” vs. “two-hands”).\n",
    "    Or set them to NaN and drop/impute them if truly missing.\n",
    "\n",
    "For now, let’s keep them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aede907-7772-41b7-ba96-aa0dfd446280",
   "metadata": {},
   "source": [
    "2.4: Normalization (Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "id": "0fed79f7-c945-4651-85d8-7aa7a8acfde8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T07:43:09.069840Z",
     "start_time": "2025-02-03T07:43:03.307687Z"
    }
   },
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # shape remains [num_samples, 126]"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0974078d-21cc-402e-849d-c154542e7022",
   "metadata": {},
   "source": [
    "2.5: Encode Labels (If Needed)"
   ]
  },
  {
   "cell_type": "code",
   "id": "84ad8a11-abd7-445d-997f-47aed653c011",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T07:43:17.080159Z",
     "start_time": "2025-02-03T07:43:16.913487Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)  # e.g., unknown -> 0, hello -> 1, ...\n",
    "num_classes = len(le.classes_)\n"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8c616d43-a6ae-499e-ac59-e786da4a8319",
   "metadata": {},
   "source": [
    "3. Split into Train/Validation/Test\n",
    "Train set: ~70%\n",
    "Validation set: ~15%\n",
    "Test set: ~15%"
   ]
  },
  {
   "cell_type": "code",
   "id": "d6d23863-f06e-4d21-a189-7dd4fd287ed3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T07:43:25.373436Z",
     "start_time": "2025-02-03T07:43:22.217023Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_scaled, y_enc, test_size=0.3, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "19e3c0af-cad2-451c-b0bd-d52936752a9d",
   "metadata": {},
   "source": [
    "4. Create a PyTorch Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "id": "c2d384f7-4231-475f-8a3b-1a9e9449c11f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T07:43:35.180869Z",
     "start_time": "2025-02-03T07:43:32.941790Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SignDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()    # shape: [N, 126]\n",
    "        self.y = torch.from_numpy(y).long()     # shape: [N]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = SignDataset(X_train, y_train)\n",
    "val_dataset   = SignDataset(X_val, y_val)\n",
    "test_dataset  = SignDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32)\n"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2fc5f66b-e835-4550-bcaa-5880373830e2",
   "metadata": {},
   "source": [
    "5. Build a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e93b591-3e75-402b-b07c-628b75a95c81",
   "metadata": {},
   "source": [
    "Because this is static data (just one row per sample), you have two main approaches:\n",
    "\n",
    "(Simpler) Treat the entire 126D input as one vector, feed it into a simple MLP or a small Transformer block that sees each “landmark” as a pseudo‐time step of length 42 per hand, etc.\n",
    "(More “Transformer-like”) Reshape your data as a sequence of length 42 (if each hand has 21 landmarks, each landmark = (x, y, z) => an embedding dimension of 3?), and pass it into a 1D Transformer encoder. This can capture some relationships among landmarks.\n",
    "Below is an example Transformer that treats each landmark as a “token.” We’ll do:\n",
    "\n",
    "For 2 hands, 42 “tokens” (21 landmarks × 2).\n",
    "Each “token” is (x, y, z) → we’ll embed from dimension 3 up to, say, 32 or 64."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d116c12-5d7b-44a1-9ed5-f011be012303",
   "metadata": {},
   "source": [
    "5.1 Reshape X to [Batch, Seq_Len, Features]\n",
    "We have 126 features per sample → each hand has 21 * 3 = 63 → 2 hands = 126 total.\n",
    "\n",
    "Seq Length = 42 (21 landmarks × 2 hands).\n",
    "Per “token” features = 3 (x, y, z).\n",
    "Thus, each row [126] can be reshaped into [42, 3]. We’ll do this inside the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5182ef-6a8b-4e5d-a8d9-1fc7db6376a0",
   "metadata": {},
   "source": [
    "5.2 Define the Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "id": "80b298f0-ef3b-45f2-bcc6-d141ea81384d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T07:43:50.699491Z",
     "start_time": "2025-02-03T07:43:50.673454Z"
    }
   },
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # shape: [1, max_len, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, d_model]\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return x\n"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cbc847e2-8145-4b4c-b10a-9e9d8b17b7f9",
   "metadata": {},
   "source": [
    "5.3 Define an Encoder-Only Transformer"
   ]
  },
  {
   "cell_type": "code",
   "id": "5f8d8d91-4e04-4814-b577-e292772ecde0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T07:44:48.107121Z",
     "start_time": "2025-02-03T07:44:48.070640Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Self-attention\n",
    "        src2, _ = self.self_attn(src, src, src)  \n",
    "        src = src + src2                  # residual\n",
    "        src = self.norm1(src)            # layer norm\n",
    "\n",
    "        # Feed-forward\n",
    "        src2 = self.linear2(self.dropout(self.act(self.linear1(src))))\n",
    "        src = src + src2                  # residual\n",
    "        src = self.norm2(src)\n",
    "        \n",
    "        return src\n",
    "\n",
    "class SignEncoder(nn.Module):\n",
    "    def __init__(self, d_model=64, n_layers=4, nhead=4, num_classes=10, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # We'll embed the (x,y,z) => d_model\n",
    "        self.input_dim = 3\n",
    "        self.seq_len   = 42  # (21 landmarks * 2 hands)\n",
    "        self.d_model   = d_model\n",
    "\n",
    "        # Project from 3 -> d_model\n",
    "        self.embedding = nn.Linear(self.input_dim, d_model)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len=self.seq_len)\n",
    "\n",
    "        # Stack multiple encoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, nhead, dim_feedforward=2*d_model, dropout=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # Final classification\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc_out = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: [batch_size, 126]\n",
    "        We'll reshape to [batch_size, 42, 3].\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # 1) reshape\n",
    "        x = x.view(batch_size, self.seq_len, self.input_dim)  # => [B, 42, 3]\n",
    "\n",
    "        # 2) embed each \"token\"\n",
    "        x = self.embedding(x)  # [B, 42, d_model]\n",
    "\n",
    "        # 3) add positional encoding\n",
    "        x = self.pos_encoding(x)  # [B, 42, d_model]\n",
    "\n",
    "        # 4) pass through encoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)  # [B, 42, d_model]\n",
    "\n",
    "        # 5) global average pool => [B, d_model]\n",
    "        # for AdaptiveAvgPool1d, we need [B, d_model, 42]\n",
    "        x = x.transpose(1, 2)  # => [B, d_model, 42]\n",
    "        x = self.global_pool(x).squeeze(-1)  # => [B, d_model]\n",
    "\n",
    "        # 6) classification\n",
    "        logits = self.fc_out(x)  # => [B, num_classes]\n",
    "        return logits\n",
    "        \n",
    "# Note: We use LayerNorm after each sub-layer (pre–post combos vary by implementation). The above has residual → norm structure, which is a common “post-norm” layout."
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "85547ca7-8b26-4eeb-a172-a84d70188e00",
   "metadata": {},
   "source": [
    "6. Initialize & Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49de59f5-ca32-47a7-8251-3f134dfb82a9",
   "metadata": {},
   "source": [
    "num_classes = num_classes  # from label encoder\n",
    "model = SignEncoder(\n",
    "    d_model=64, \n",
    "    n_layers=4, \n",
    "    nhead=4, \n",
    "    num_classes=num_classes, \n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)      # [batch_size, num_classes]\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Accuracy\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == batch_y).sum().item()\n",
    "            total += len(batch_y)\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Val Acc: {val_acc:.4f}\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef4a26bd-4719-4fab-8051-25ff022e1ee8",
   "metadata": {},
   "source": [
    "torch.save(model.state_dict(), \"sign_language_encoder_model.pth\")\n",
    "print(\"✅ Model saved successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7282f80a-ea67-4a3e-9da7-57ad9480f97b",
   "metadata": {},
   "source": [
    "6.1 tunnig model for better accuray "
   ]
  },
  {
   "cell_type": "code",
   "id": "95c6b2c7-d4e3-46c5-8970-ef0c67d2545f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T08:00:26.867248Z",
     "start_time": "2025-02-03T07:48:12.892491Z"
    }
   },
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Let's assume you have:\n",
    "# 1) model = SignEncoder(...)\n",
    "# 2) train_loader, val_loader\n",
    "# 3) device, etc.\n",
    "num_classes = num_classes  # from label encoder\n",
    "model = SignEncoder(\n",
    "    d_model=64, \n",
    "    n_layers=4, \n",
    "    nhead=4, \n",
    "    num_classes=num_classes, \n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Load the previously trained weights\n",
    "model.load_state_dict(torch.load(\"sign_language_encoder_model - extra.pth\"))\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# Use a learning rate scheduler that reduces LR when validation acc plateaus\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=2, verbose= False\n",
    ")\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_val_acc = 0.0\n",
    "no_improve_count = 0\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct += (preds == batch_y).sum().item()\n",
    "            total += len(batch_y)\n",
    "    \n",
    "    val_acc = correct / total\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Update LR scheduler (monitor val_acc, so mode='max')\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Val Acc: {val_acc:.4f}, \"\n",
    "          f\"Current LR: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        no_improve_count = 0\n",
    "    else:\n",
    "        no_improve_count += 1\n",
    "        if no_improve_count >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best weights and save\n",
    "model.load_state_dict(best_model_wts)\n",
    "torch.save(model.state_dict(), \"best_encoder_model.pth\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "33b8a447-c533-49d8-b593-a7af46b813b5",
   "metadata": {},
   "source": [
    "7. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "508b054f-264f-47c6-97bb-ee85240b8a92",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == batch_y).sum().item()\n",
    "        total += len(batch_y)\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "test_acc = correct / total\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cb9135bf-0ac9-47f0-8bf3-4d38ad9edd1d",
   "metadata": {},
   "source": [
    "8. Inference on New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097a1954-9996-488d-8f9d-2fad00560902",
   "metadata": {},
   "source": [
    "When you have a new CSV row:\n",
    "\n",
    "Parse it as a (1,126) tensor (assuming 2 hands, 21 landmarks each, x/y/z).\n",
    "Apply the same scaler used during training.\n",
    "Pass it to model.eval() → get predictions → map back to class label with le.inverse_transform().\n",
    "\n",
    "Example code:\n",
    "model.eval()\n",
    "single_row = ...  # shape [126], from new CSV row\n",
    "single_row_scaled = scaler.transform(single_row.reshape(1, -1))\n",
    "single_tensor = torch.from_numpy(single_row_scaled).float().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(single_tensor)  # shape [1, num_classes]\n",
    "    pred_class_id = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "pred_label = le.inverse_transform([pred_class_id])[0]\n",
    "print(\"Predicted sign:\", pred_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f2f2e-2831-437c-b1db-844c05c95f8a",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
