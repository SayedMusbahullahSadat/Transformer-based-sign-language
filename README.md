# Transformer-based-sign-language
Developed an AI-powered sign language recognition system to bridge communication gaps between Deaf and hearing individuals. The system first focused on isolated-sign recognition using the WLASL dataset (2,000 ASL glosses) and MediaPipe to extract 3D hand landmarks. A custom Transformer encoder was trained on this data, achieving ~65% accuracy using cross-entropy loss, Adam optimizer, and early stopping. A demo GUI built with Tkinter allows users to upload a video and receive the predicted gloss label in real-time. The project was extended to continuous sign recognition using a custom dataset generator that concatenates isolated clips with noise and variation for realism. A Transformer-based ContinuousSignEncoder model was trained using PyTorchâ€™s CTC loss to support sequence decoding without exact alignments. Validation metrics included token accuracy, sequence accuracy, and word error rate (WER). later improvements include beam-search decoding, and a mobile-friendly GUI, aiming for a deployable, real-time translation tool for everyday use.
